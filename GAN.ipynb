{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayCXC/NSF-REU-2021/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbvZ-d105LLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a7f80f-4f52-4c82-bb3b-7a26f4ada63e"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Xtst.csv'):\n",
        "  !gdown --id 1wQDp91LC0PBJvJRBTLw1aW51-UN5JOm8\n",
        "if not os.path.exists('Ytr.csv'):\n",
        "  !gdown --id 1ZhPVjXlOttrYRjSRm_IUBZj-Bokswzbn\n",
        "if not os.path.exists('Ytst.csv'):\n",
        "  !gdown --id 10LE7A7yhNkPlk8Zv3dmBjFYwMW9ZKy9_\n",
        "if not os.path.exists('Xtr.csv'):\n",
        "  !gdown --id 1IUcv_OcLX1YKUB1Xw_lYiBK6bnyLxikS\n",
        "if not os.path.exists('cnn1d.zip'):\n",
        "  !curl --remote-name -H 'Accept: application/vnd.github.v3.raw' --location 'https://github.com/MayCXC/NSF-REU-2021/raw/main/cnn1d.zip'\n",
        "#if not os.path.exists('discriminator.zip'):\n",
        "#  !curl --remote-name -H 'Accept: application/vnd.github.v3.raw' --location 'https://github.com/MayCXC/NSF-REU-2021/raw/main/discriminator.zip'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wQDp91LC0PBJvJRBTLw1aW51-UN5JOm8\n",
            "To: /content/Xtst.csv\n",
            "308MB [00:02, 129MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZhPVjXlOttrYRjSRm_IUBZj-Bokswzbn\n",
            "To: /content/Ytr.csv\n",
            "25.7MB [00:00, 149MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LE7A7yhNkPlk8Zv3dmBjFYwMW9ZKy9_\n",
            "To: /content/Ytst.csv\n",
            "6.42MB [00:00, 99.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IUcv_OcLX1YKUB1Xw_lYiBK6bnyLxikS\n",
            "To: /content/Xtr.csv\n",
            "1.23GB [00:12, 97.7MB/s]\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   134  100   134    0     0    482      0 --:--:-- --:--:-- --:--:--   482\n",
            "100 1232k  100 1232k    0     0  2107k      0 --:--:-- --:--:-- --:--:-- 2107k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKcJ9_rYIpi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfba11c3-d972-4dee-f95e-c1a6b24409b8"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "Xtr = pd.read_csv(\"Xtr.csv\", sep=',', header=None)\n",
        "Xtst = pd.read_csv(\"Xtst.csv\", sep=',', header=None)\n",
        "Ytr = pd.read_csv(\"Ytr.csv\", sep=',', header=None)\n",
        "Ytst = pd.read_csv(\"Ytst.csv\", sep=',', header=None)\n",
        "\n",
        "def train_test_base_model(Xtr, Xtst, Ytr, Ytst, epochs):\n",
        "  scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "  xtr = scaler.transform(Xtr)  \n",
        "  xtst = scaler.transform(Xtst)  \n",
        "\n",
        "  xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "  xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "  print (\"Number of training samples is: \",xtr.shape[0])\n",
        "  print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "  num_classes = 2\n",
        "\n",
        "  ytr = to_categorical(Ytr, num_classes)\n",
        "  ytst = to_categorical(Ytst, num_classes)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(32, kernel_size=7, activation='relu', input_shape=xtr.shape[1:]))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))    \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "  model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.3)  #learning_rte\n",
        "  print(\"Evaluation of best performing model:\")\n",
        "  model.evaluate(xtst, ytst)\n",
        "  return model\n",
        "\n",
        "if not os.path.exists('cnn1d.zip'):\n",
        "  model = train_test_base_model(Xtr,Xtst,Ytr,Ytst,150)\n",
        "  model.save('cnn1d')\n",
        "  !zip -r cnn1d.zip cnn1d\n",
        "else:\n",
        "  if not os.path.exists('cnn1d'):\n",
        "    !unzip cnn1d.zip\n",
        "  model = load_model('cnn1d')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cnn1d.zip\n",
            "   creating: cnn1d/\n",
            "   creating: cnn1d/variables/\n",
            "  inflating: cnn1d/variables/variables.index  \n",
            "  inflating: cnn1d/variables/variables.data-00000-of-00001  \n",
            "   creating: cnn1d/assets/\n",
            "  inflating: cnn1d/saved_model.pb    \n",
            "  inflating: cnn1d/keras_metadata.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfYlZAxMom76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7747e92c-478a-467a-afa2-d97ba6e3cde6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def percentiles(x,y,p):\n",
        "  benign = np.compress(y == 0.0, x, axis=0)\n",
        "  totals = np.sum(benign, axis=1)\n",
        "  return (y == 0.) & (np.sum(x, axis=1) < np.percentile(totals,p))\n",
        "\n",
        "Ytr_low = percentiles(Xtr.to_numpy(),Ytr.to_numpy().flatten(),10).astype(int)\n",
        "Ytst_low = percentiles(Xtst.to_numpy(),Ytst.to_numpy().flatten(),10).astype(int)\n",
        "\n",
        "if not os.path.exists('generator.zip'):\n",
        "  generator = train_test_base_model(Xtr, Xtst, Ytr_low, Ytst_low, 10)\n",
        "  generator.save('generator')\n",
        "  !zip -r generator.zip generator\n",
        "else:\n",
        "  if not os.path.exists('generator'):\n",
        "    !unzip generator.zip\n",
        "  generator = load_model('generator')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples is:  1029085\n",
            "Number of test samples is:  256883\n",
            "Epoch 1/10\n",
            "5628/5628 - 52s - loss: 0.1099 - accuracy: 0.9524 - val_loss: 0.1319 - val_accuracy: 0.9463\n",
            "Epoch 2/10\n",
            "5628/5628 - 51s - loss: 0.0901 - accuracy: 0.9604 - val_loss: 0.1459 - val_accuracy: 0.9484\n",
            "Epoch 3/10\n",
            "5628/5628 - 52s - loss: 0.0728 - accuracy: 0.9698 - val_loss: 0.1557 - val_accuracy: 0.9538\n",
            "Epoch 4/10\n",
            "5628/5628 - 52s - loss: 0.0608 - accuracy: 0.9755 - val_loss: 0.1697 - val_accuracy: 0.9545\n",
            "Epoch 5/10\n",
            "5628/5628 - 52s - loss: 0.0544 - accuracy: 0.9781 - val_loss: 0.1775 - val_accuracy: 0.9566\n",
            "Epoch 6/10\n",
            "5628/5628 - 52s - loss: 0.0495 - accuracy: 0.9802 - val_loss: 0.1789 - val_accuracy: 0.9505\n",
            "Epoch 7/10\n",
            "5628/5628 - 51s - loss: 0.0460 - accuracy: 0.9816 - val_loss: 0.2017 - val_accuracy: 0.9559\n",
            "Epoch 8/10\n",
            "5628/5628 - 51s - loss: 0.0438 - accuracy: 0.9824 - val_loss: 0.2060 - val_accuracy: 0.9561\n",
            "Epoch 9/10\n",
            "5628/5628 - 50s - loss: 0.0417 - accuracy: 0.9834 - val_loss: 0.2613 - val_accuracy: 0.9567\n",
            "Epoch 10/10\n",
            "5628/5628 - 50s - loss: 0.0395 - accuracy: 0.9841 - val_loss: 0.2079 - val_accuracy: 0.9581\n",
            "Evaluation of best performing model:\n",
            "8028/8028 [==============================] - 13s 2ms/step - loss: 0.1104 - accuracy: 0.9696\n",
            "INFO:tensorflow:Assets written to: generator/assets\n",
            "  adding: generator/ (stored 0%)\n",
            "  adding: generator/assets/ (stored 0%)\n",
            "  adding: generator/saved_model.pb (deflated 88%)\n",
            "  adding: generator/variables/ (stored 0%)\n",
            "  adding: generator/variables/variables.data-00000-of-00001 (deflated 15%)\n",
            "  adding: generator/variables/variables.index (deflated 64%)\n",
            "  adding: generator/keras_metadata.pb (deflated 89%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ-6s2n6-yxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75255614-1350-4854-9ac5-c596331e629d"
      },
      "source": [
        "try:\n",
        "  from art.attacks.evasion import FastGradientMethod, Wasserstein\n",
        "  from art.estimators.classification import TensorFlowV2Classifier\n",
        "except ImportError:\n",
        "  from sys import executable\n",
        "  !{executable} -m pip install adversarial-robustness-toolbox[Keras]\n",
        "  from art.attacks.evasion import FastGradientMethod, Wasserstein\n",
        "  from art.estimators.classification import TensorFlowV2Classifier\n",
        "import tensorflow as tf\n",
        "\n",
        "# Train the ART classifier\n",
        "\n",
        "def keras_classifier_art(model,nb_classes,shape):\n",
        "  return TensorFlowV2Classifier(\n",
        "      model=model,\n",
        "      nb_classes=nb_classes,\n",
        "      input_shape=shape,\n",
        "      loss_object=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "  #      label_smoothing=0,\n",
        "  #      axis=-1,\n",
        "  #      reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"\n",
        "    )\n",
        "  )\n",
        "\n",
        "\n",
        "def attack_art(discriminator, generator):\n",
        "  scaler = preprocessing.StandardScaler().fit(Xtst)\n",
        "  xtst = scaler.transform(Xtst).reshape(*Xtst.shape,1)\n",
        "  ytst = to_categorical(Ytst, 2)\n",
        "\n",
        "  discriminator_classifier = keras_classifier_art(discriminator,2,xtst.shape)\n",
        "  predictions = discriminator_classifier.predict(xtst)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(ytst, axis=1)) / len(ytst)\n",
        "  print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
        "\n",
        "  generator_classifier = keras_classifier_art(generator,2,xtst.shape)\n",
        "\n",
        "  # Generate adversarial test examples\n",
        "  attack = FastGradientMethod(estimator=generator_classifier, eps=0.2)\n",
        "  #attack = Wasserstein(estimator=classifier, eps=0.2)\n",
        "  xattack = attack.generate(x=xtst)\n",
        "\n",
        "  # Evaluate the ART classifier on adversarial test examples\n",
        "\n",
        "  predictions = discriminator_classifier.predict(xattack)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(ytst, axis=1)) / len(ytst)\n",
        "  print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))\n",
        "  print(\"Mean consumption of benign data: \" + str(np.mean(xtst)))\n",
        "  print(\"Mean consumption of adversarial test examples: \" + str(np.mean(xattack)))\n",
        "\n",
        "attack_art(model, generator)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on benign test examples: 90.24575390352807%\n",
            "Accuracy on adversarial test examples: 83.97363780398081%\n",
            "Mean consumption of benign data: 1.5240753471573253e-17\n",
            "Mean consumption of adversarial test examples: -0.022997515\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
