{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayhd3/NSF-REU-2021/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbvZ-d105LLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32fb789-8b6c-4054-ef91-36d855b82d82"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Xtst.csv'):\n",
        "  !gdown --id 1wQDp91LC0PBJvJRBTLw1aW51-UN5JOm8\n",
        "if not os.path.exists('Ytr.csv'):\n",
        "  !gdown --id 1ZhPVjXlOttrYRjSRm_IUBZj-Bokswzbn\n",
        "if not os.path.exists('Ytst.csv'):\n",
        "  !gdown --id 10LE7A7yhNkPlk8Zv3dmBjFYwMW9ZKy9_\n",
        "if not os.path.exists('Xtr.csv'):\n",
        "  !gdown --id 1IUcv_OcLX1YKUB1Xw_lYiBK6bnyLxikS\n",
        "if not os.path.exists('cnn1d.zip'):\n",
        "  !curl --remote-name -H 'Accept: application/vnd.github.v3.raw' --location 'https://github.com/mayhd3/NSF-REU-2021/raw/main/cnn1d.zip'\n",
        "#if not os.path.exists('discriminator.zip'):\n",
        "#  !curl --remote-name -H 'Accept: application/vnd.github.v3.raw' --location 'https://github.com/mayhd3/NSF-REU-2021/raw/main/discriminator.zip'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wQDp91LC0PBJvJRBTLw1aW51-UN5JOm8\n",
            "To: /content/Xtst.csv\n",
            "308MB [00:16, 19.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZhPVjXlOttrYRjSRm_IUBZj-Bokswzbn\n",
            "To: /content/Ytr.csv\n",
            "25.7MB [00:00, 38.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10LE7A7yhNkPlk8Zv3dmBjFYwMW9ZKy9_\n",
            "To: /content/Ytst.csv\n",
            "6.42MB [00:00, 24.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IUcv_OcLX1YKUB1Xw_lYiBK6bnyLxikS\n",
            "To: /content/Xtr.csv\n",
            "1.23GB [00:25, 47.7MB/s]\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   134  100   134    0     0    341      0 --:--:-- --:--:-- --:--:--   340\n",
            "100 1232k  100 1232k    0     0  1566k      0 --:--:-- --:--:-- --:--:-- 1566k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKcJ9_rYIpi5",
        "outputId": "09c943af-8842-4f1d-fe5c-5ac22ce82078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "Xtr = pd.read_csv(\"Xtr.csv\", sep=',', header=None)\n",
        "Xtst = pd.read_csv(\"Xtst.csv\", sep=',', header=None)\n",
        "Ytr = pd.read_csv(\"Ytr.csv\", sep=',', header=None)\n",
        "Ytst = pd.read_csv(\"Ytst.csv\", sep=',', header=None)\n",
        "\n",
        "def train_test_base_model(Xtr, Xtst, Ytr, Ytst, epochs):\n",
        "  scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "  xtr = scaler.transform(Xtr)  \n",
        "  xtst = scaler.transform(Xtst)  \n",
        "\n",
        "  xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "  xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "  print (\"Number of training samples is: \",xtr.shape[0])\n",
        "  print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "  num_classes = 2\n",
        "\n",
        "  ytr = to_categorical(Ytr, num_classes)\n",
        "  ytst = to_categorical(Ytst, num_classes)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(32, kernel_size=7, activation='relu', input_shape=xtr.shape[1:]))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))    \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "  model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.3)  #learning_rte\n",
        "  print(\"Evaluation of best performing model:\")\n",
        "  model.evaluate(xtst, ytst)\n",
        "  return model\n",
        "\n",
        "if not os.path.exists('cnn1d.zip'):\n",
        "  model = train_test_base_model(Xtr,Xtst,Ytr,Ytst,150)\n",
        "  model.save('cnn1d')\n",
        "  !zip -r cnn1d.zip cnn1d\n",
        "else:\n",
        "  if not os.path.exists('cnn1d'):\n",
        "    !unzip cnn1d.zip\n",
        "  model = load_model('cnn1d')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cnn1d.zip\n",
            "   creating: cnn1d/\n",
            "   creating: cnn1d/variables/\n",
            "  inflating: cnn1d/variables/variables.index  \n",
            "  inflating: cnn1d/variables/variables.data-00000-of-00001  \n",
            "   creating: cnn1d/assets/\n",
            "  inflating: cnn1d/saved_model.pb    \n",
            "  inflating: cnn1d/keras_metadata.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfYlZAxMom76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1364f4-6755-45e5-bed3-4dac324bd304"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def percentiles(x,y,p):\n",
        "# labels = np.argmax(y, axis=1)\n",
        "  bisect = [ np.compress(y == label, x, axis=0) for label in [0.0,1.0] ]\n",
        "  print(x.shape)\n",
        "  totals = [ np.sum(part, axis=1) for part in bisect ]\n",
        "  print([np.percentile(total, p) for total in totals])\n",
        "  return (y == 0.) & (np.sum(x, axis=1) < np.percentile(totals[0],p))\n",
        "    # [bisect[i][totals[i] < np.percentile(totals[i], p)] for i in [0,1]]\n",
        "\n",
        "Ytr_low = percentiles(Xtr.to_numpy(),Ytr.to_numpy().flatten(),10).astype(int)\n",
        "Ytst_low = percentiles(Xtst.to_numpy(),Ytst.to_numpy().flatten(),10).astype(int)\n",
        "\n",
        "if not os.path.exists('discriminator.zip'):\n",
        "  discriminator = train_test_base_model(Xtr, Xtst, Ytr_low, Ytst_low, 10)\n",
        "  discriminator.save('discriminator')\n",
        "  !zip -r discriminator.zip discriminator\n",
        "else:\n",
        "  if not os.path.exists('discriminator'):\n",
        "    !unzip discriminator.zip\n",
        "  discriminator = load_model('discriminator')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1029085, 48)\n",
            "[5.958668296039206, 2.203336569764192]\n",
            "(256883, 48)\n",
            "[6.358757727843121, 2.561238887802642]\n",
            "Number of training samples is:  1029085\n",
            "Number of test samples is:  256883\n",
            "Epoch 1/10\n",
            "5628/5628 - 72s - loss: 0.1101 - accuracy: 0.9526 - val_loss: 0.1287 - val_accuracy: 0.9482\n",
            "Epoch 2/10\n",
            "5628/5628 - 53s - loss: 0.0878 - accuracy: 0.9624 - val_loss: 0.1361 - val_accuracy: 0.9513\n",
            "Epoch 3/10\n",
            "5628/5628 - 53s - loss: 0.0707 - accuracy: 0.9713 - val_loss: 0.1296 - val_accuracy: 0.9547\n",
            "Epoch 4/10\n",
            "5628/5628 - 53s - loss: 0.0622 - accuracy: 0.9751 - val_loss: 0.1418 - val_accuracy: 0.9532\n",
            "Epoch 5/10\n",
            "5628/5628 - 53s - loss: 0.0566 - accuracy: 0.9774 - val_loss: 0.1393 - val_accuracy: 0.9589\n",
            "Epoch 6/10\n",
            "5628/5628 - 52s - loss: 0.0522 - accuracy: 0.9794 - val_loss: 0.1537 - val_accuracy: 0.9548\n",
            "Epoch 7/10\n",
            "5628/5628 - 54s - loss: 0.0486 - accuracy: 0.9807 - val_loss: 0.1649 - val_accuracy: 0.9579\n",
            "Epoch 8/10\n",
            "5628/5628 - 56s - loss: 0.0458 - accuracy: 0.9817 - val_loss: 0.1687 - val_accuracy: 0.9582\n",
            "Epoch 9/10\n",
            "5628/5628 - 53s - loss: 0.0437 - accuracy: 0.9827 - val_loss: 0.1711 - val_accuracy: 0.9597\n",
            "Epoch 10/10\n",
            "5628/5628 - 52s - loss: 0.0419 - accuracy: 0.9832 - val_loss: 0.1881 - val_accuracy: 0.9581\n",
            "Evaluation of best performing model:\n",
            "8028/8028 [==============================] - 15s 2ms/step - loss: 0.1033 - accuracy: 0.9719\n",
            "INFO:tensorflow:Assets written to: discriminator/assets\n",
            "  adding: discriminator/ (stored 0%)\n",
            "  adding: discriminator/variables/ (stored 0%)\n",
            "  adding: discriminator/variables/variables.data-00000-of-00001 (deflated 17%)\n",
            "  adding: discriminator/variables/variables.index (deflated 64%)\n",
            "  adding: discriminator/saved_model.pb (deflated 87%)\n",
            "  adding: discriminator/assets/ (stored 0%)\n",
            "  adding: discriminator/keras_metadata.pb (deflated 89%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ-6s2n6-yxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "c449eaee-3798-4458-8485-c5313ef1ba3c"
      },
      "source": [
        "try:\n",
        "  from art.attacks.evasion import FastGradientMethod, Wasserstein\n",
        "  from art.estimators.classification import TensorFlowV2Classifier\n",
        "except ImportError:\n",
        "  from sys import executable\n",
        "  !{executable} -m pip install adversarial-robustness-toolbox[Keras]\n",
        "  from art.attacks.evasion import FastGradientMethod, Wasserstein\n",
        "  from art.estimators.classification import TensorFlowV2Classifier\n",
        "import tensorflow as tf\n",
        "\n",
        "# Train the ART classifier\n",
        "\n",
        "def attack(model, xtst):\n",
        "  scaler = preprocessing.StandardScaler().fit(xtst)\n",
        "  xtest = scaler.transform(xtst).reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "  classifier = TensorFlowV2Classifier(\n",
        "      model=model,\n",
        "      nb_classes=2,\n",
        "      input_shape=xtest.shape,\n",
        "      loss_object=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "  #      label_smoothing=0,\n",
        "  #      axis=-1,\n",
        "  #      reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"\n",
        "    )\n",
        "  )\n",
        "  predictions = classifier.predict(xtest)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(ytst, axis=1)) / len(ytst)\n",
        "  print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
        "\n",
        "  # Generate adversarial test examples\n",
        "#  attack = FastGradientMethod(estimator=classifier, eps=0.2)\n",
        "  attack = Wasserstein(estimator=classifier, eps=0.2)\n",
        "  xattack = attack.generate(x=xtest)\n",
        "\n",
        "  # Evaluate the ART classifier on adversarial test examples\n",
        "\n",
        "  predictions = classifier.predict(xattack)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(ytst, axis=1)) / len(ytst)\n",
        "  print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))\n",
        "\n",
        "attack(model, Xtst)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox[Keras]\n",
            "  Downloading adversarial_robustness_toolbox-1.7.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (57.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (4.41.1)\n",
            "Collecting numba~=0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.24.3,>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox[Keras]) (2.4.3)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 81 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24.3,>=0.22.2->adversarial-robustness-toolbox[Keras]) (1.0.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->adversarial-robustness-toolbox[Keras]) (1.5.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->adversarial-robustness-toolbox[Keras]) (3.13)\n",
            "Installing collected packages: llvmlite, numba, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "Successfully installed adversarial-robustness-toolbox-1.7.1 llvmlite-0.36.0 numba-0.53.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2057aba8d621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on adversarial test examples: {}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mattack_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastGradientMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYTW_P-OX4hd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}