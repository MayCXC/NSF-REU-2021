{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepdata.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3m+5jmHA3qrXQq9klt2lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayCXC/NSF-REU-2021/blob/main/prepdata6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGzgpdTx_30N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c395835-39bd-486f-d81e-2302cca28a36"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Irish_dataset.csv'):\n",
        "  !gdown --id 1OFQo6CmBPwn3FRB5EsRm9t6wba_5Lcr9"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OFQo6CmBPwn3FRB5EsRm9t6wba_5Lcr9\n",
            "To: /content/Irish_dataset.csv\n",
            "877MB [00:04, 178MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOYFkbGcovls",
        "outputId": "43c05a32-8b2d-4b79-9ebf-0646806d1342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM, Merge, GRU\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.utils import shuffle\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "i = 0 # counter for customers\n",
        "a_dr = 0 # detection rate initialization\n",
        "a_fr = 0 # false rejection initialization \n",
        "a_fa = 0 # false acceptance initialization\n",
        "fIn = \"Irish_dataset.csv\" \n",
        "# fOt1 = sys.argv[2]\n",
        "np.random.seed(1234) # Set the random seed for numpy\n",
        "# flog1 = open(fOt1,'w')\n",
        "scores = list()\n",
        "\n",
        "# creat six types of attacks\n",
        "def Creatattacks(d):\n",
        "\talpha1 = random.uniform(0.1,0.6)\n",
        "\tat1 = np.dot(alpha1,d) #1st attack type\n",
        "\tit = np.random.randint(0, high=42, size=536)\n",
        "\tdu = np.random.randint(8, high=48, size=536)\n",
        "\tfl = np.minimum(it + du,48) \n",
        "\tat2 = copy.deepcopy(d)\n",
        "\tfor l in np.arange(0,536):\n",
        "\t\tat2[l,it[l]:fl[l]]=0 #2nd attack type\n",
        "\talpha2 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat3 = np.multiply(alpha2,d) # 3rd attack type\n",
        "\tavg = np.mean(d,axis=1) # customer daily average consumption\n",
        "\tavg_m = np.diag(avg) # in diagonal matrix for multiplication  \n",
        "\talpha3 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat4 = np.dot(avg_m,alpha3) # 4th attack type\n",
        "\talpha4 = np.ones((536,48))\n",
        "\tat5 = np.dot(avg_m,alpha4) # 5th attack type\n",
        "\tat6 = np.fliplr(d) # 6th attack type \n",
        "\treturn at1,at2,at3,at4,at5,at6\n",
        "\n",
        "# Split the data to train and test\n",
        "def Split_Train_Test(X_res,Y_res):\n",
        "\tind1 = np.where(Y_res == 0)[0]\n",
        "\tXH = X_res[ind1,:] # honest samples\n",
        "\tYH = np.zeros(ind1.shape[0])\n",
        "\tind_H = int(math.ceil(ind1.shape[0]*4/5))\n",
        "\tXH_TR = XH[0:ind_H,:]\n",
        "\tYH_TR = YH[0:ind_H]\n",
        "\tXH_TST = XH[ind_H:,:]\n",
        "\tYH_TST = YH[ind_H:] \n",
        "\tXC = np.concatenate((at1,at2,at3,at4,at5,at6), axis=0) # all attacks\n",
        "\tYC = np.ones(XC.shape[0]) \n",
        "\tind_C = int(math.ceil(536*4/5))\n",
        "\tXC_TR = np.concatenate((at1[0:ind_C,:],at2[0:ind_C,:],at3[0:ind_C,:],at4[0:ind_C,:],at5[0:ind_C,:],at6[0:ind_C,:]), axis=0) # training cheating samples\n",
        "\tYC_TR = YC[0:XC_TR.shape[0]]\n",
        "\tXC_TST = np.concatenate((at1[ind_C:,:],at2[ind_C:,:],at3[ind_C:,:],at4[ind_C:,:],at5[ind_C:,:],at6[ind_C:,:]), axis=0) # test cheating samples\n",
        "\tYC_TST = YC[XC_TR.shape[0]:]     \n",
        "\tXTR = np.concatenate((XH_TR,XC_TR), axis=0)  \n",
        "\tYTR = np.concatenate((YH_TR,YC_TR), axis=0) \n",
        "\tind = list(range(XTR.shape[0]))  #random shuffling the training data\n",
        "\tnp.random.shuffle(ind)\n",
        "\tXTR = XTR[ind]\n",
        "\tYTR = YTR[ind]\n",
        "\tXTST = np.concatenate((XH_TST,XC_TST), axis=0)   \n",
        "\tYTST = np.concatenate((YH_TST,YC_TST), axis=0)   \n",
        "\treturn XTR,YTR,XTST,YTST\n",
        "\n",
        "print (\"Loading Data ...\")\n",
        "#\tD = np.array(list(csv.reader(open(fIn, \"rt\"), delimiter=\",\"))).astype(\"float\")\n",
        "D = np.loadtxt(fIn, delimiter=\",\")\n",
        "D = np.transpose(D[1:,:])\n",
        "x_tr = open('Xtr_all_200.csv','ab')\n",
        "y_tr = open('Ytr_all_200.csv','ab')\n",
        "x_tst = open('Xtst_all_200.csv','ab')\n",
        "y_tst = open('Ytst_all_200.csv','ab')\n",
        "\n",
        "for d in D: # for each user\n",
        "  if i==200:\n",
        "    break\n",
        "  i+=1\n",
        "  if i%10==0:\n",
        "    print(i)\n",
        "  file_name = 'Users/'+str(i)+'.pkl'\n",
        "  #if training data for that user exist load it (balanced data set for each user has both honest and malicious samples) \n",
        "  if os.path.exists(file_name):\n",
        "    [XTR,YTR,XTST,YTST] = pickle.load(open(file_name, 'rb'))\n",
        "  else:\n",
        "    d = d.reshape(536,48) # customer data arranged in matrix: rows = days, columns = time\n",
        "      ############################################################## \n",
        "      # Creating attacks\n",
        "    at1, at2, at3, at4, at5, at6 = Creatattacks(d)\n",
        "      ##############################################################    \n",
        "      # Over-sampling\n",
        "    X = np.concatenate((d,at1,at2,at3,at4,at5,at6), axis=0) # all data = honest + attacks\n",
        "    y1 = np.zeros(536)\n",
        "    y2 = np.ones(536*6)\n",
        "    Y = np.concatenate((y1,y2), axis=0)\n",
        "    ada = ADASYN(ratio='minority',random_state=42)\n",
        "    X_res, Y_res = ada.fit_sample(X,Y) # over-sampled data\n",
        "      \n",
        "      ##############################################################    \n",
        "      # Split data to train 2/3 and test 1/3\n",
        "    XTR,YTR,XTST,YTST = Split_Train_Test(X_res,Y_res) \n",
        "    #pickle.dump([XTR,YTR,XTST,YTST], open(file_name, 'wb'))\n",
        "      ############################################################## \t\t\t\t\n",
        "  np.savetxt(x_tr, XTR, delimiter=\",\")\n",
        "  np.savetxt(y_tr, YTR, delimiter=\",\")\n",
        "  np.savetxt(x_tst, XTST, delimiter=\",\")\n",
        "  np.savetxt(y_tst, YTST, delimiter=\",\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading Data ...\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-gXSYI9K8Md",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "99cd8aeb-4cca-41c7-8314-e66c8c3ddb4a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, MaxPooling1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "num_classes = 2 \n",
        "epochs = 150     #should be set\n",
        "input_shape = (48,1)\n",
        "\n",
        "# load the data\n",
        "def data():\n",
        "    Xtr = pd.read_csv(\"Xtr_all_200.csv\", sep=',', header=None)\n",
        "    Xtst = pd.read_csv(\"Xtst_all_200.csv\", sep=',', header=None)\n",
        "    Ytr = pd.read_csv(\"Ytr_all_200.csv\", sep=',', header=None)\n",
        "    Ytst = pd.read_csv(\"Ytst_all_200.csv\", sep=',', header=None)\n",
        "    \n",
        "    print((Xtr.shape, Xtst.shape, Ytr.shape, Ytst.shape))\n",
        "\n",
        "    scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "    xtr = scaler.transform(Xtr)  \n",
        "    xtst = scaler.transform(Xtst)  \n",
        "\n",
        "    xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "    xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "    nb_classes = 2\n",
        "\n",
        "    print (\"Number of training samples is: \",xtr.shape[0])\n",
        "    print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "    ytr = to_categorical(Ytr, nb_classes)\n",
        "    ytst = to_categorical(Ytst, nb_classes)\n",
        "    \n",
        "    return xtr,ytr,xtst,ytst\n",
        "    \n",
        "xtr,ytr,xtst,ytst=data()\n",
        "\n",
        "input_shape = (48,1)\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_shape)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))    \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.3)  #learning_rte\n",
        "    \n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(xtst, ytst))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((1029828, 48), (257058, 48), (1029671, 1), (257051, 1))\n",
            "Number of training samples is:  1029828\n",
            "Number of test samples is:  257058\n",
            "Epoch 1/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bac4cb6fc56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#learning_rte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m                 steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1189\u001b[0m           val_logs = self.evaluate(\n\u001b[1;32m   1190\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1612\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1613\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 308949\n  y sizes: 308792\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il53yObaKkBl",
        "outputId": "27b5e581-1b44-4f63-95b9-7c3c7ca7ecc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# load the data\n",
        "def data():\n",
        "    Xtr = pd.read_csv(\"Xtr_all_200.csv\", sep=',', header=None)\n",
        "    Xtst = pd.read_csv(\"Xtst_all_200.csv\", sep=',', header=None)\n",
        "    Ytr = pd.read_csv(\"Ytr_all_200.csv\", sep=',', header=None)\n",
        "    Ytst = pd.read_csv(\"Ytst_all_200.csv\", sep=',', header=None)\n",
        "    \n",
        "    scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "    xtr = scaler.transform(Xtr)  \n",
        "    xtst = scaler.transform(Xtst)  \n",
        "\n",
        "    xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "    xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "    nb_classes = 2\n",
        "\n",
        "    print (\"Number of training samples is: \",xtr.shape[0])\n",
        "    print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "    ytr = to_categorical(Ytr, nb_classes)\n",
        "    ytst = to_categorical(Ytst, nb_classes)\n",
        "    \n",
        "    return xtr,ytr,xtst,ytst\n",
        "    \n",
        "xtr,ytr,xtst,ytst=data()\n",
        "\n",
        "num_classes = 2 \n",
        "epochs = 150     #should be set\n",
        "\n",
        "input_shape = (48,1)\n",
        "model = Sequential()\n",
        "model.add(GRU(32, input_shape=(input_shape), activation='relu'))  \n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))    \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.2)  #learning_rte\n",
        "\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(xtst, ytst))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-27ef8469ca89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-27ef8469ca89>\u001b[0m in \u001b[0;36mdata\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mXtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xtr_all_200.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mXtst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xtst_all_200.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mYtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ytr_all_200.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPH8j3D9nEci"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
