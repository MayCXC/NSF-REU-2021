{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepdata.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaUZD0tYWQf6NIvEFfA6xy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayhd3/NSF-REU-2021/blob/main/prepdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGzgpdTx_30N"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Irish_dataset.csv'):\n",
        "  !gdown --id 1OFQo6CmBPwn3FRB5EsRm9t6wba_5Lcr9\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6oMnJwF_u5H"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM, Merge, GRU\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.utils import shuffle\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "\n",
        "i = 0 # counter for customers\n",
        "a_dr = 0 # detection rate initialization\n",
        "a_fr = 0 # false rejection initialization \n",
        "a_fa = 0 # false acceptance initialization\n",
        "fIn = \"Irish_dataset.csv\" \n",
        "# fOt1 = sys.argv[2]\n",
        "np.random.seed(1234) # Set the random seed for numpy\n",
        "# flog1 = open(fOt1,'w')\n",
        "scores = list()\n",
        "\n",
        "\n",
        "# creat six types of attacks\n",
        "def create_attacks(d):\n",
        "\talpha1 = random.uniform(0.1,0.6)\n",
        "\tat1 = np.dot(alpha1,d) #1st attack type\t\n",
        "\talpha2 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat3 = np.multiply(alpha2,d) # 3rd attack type\n",
        "\tavg = np.mean(d,axis=1) # customer daily average consumption\n",
        "\tavg_m = np.diag(avg) # in diagonal matrix for multiplication  \n",
        "\talpha3 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat4 = np.dot(avg_m,alpha3) # 4th attack type\n",
        "\talpha4 = np.ones((536,48))\n",
        "\tat5 = np.dot(avg_m,alpha4) # 5th attack type\t\n",
        "\treturn at1,at3,at4,at5\n",
        "\n",
        "# Split the data to train and test\n",
        "def split_train_test(X_res,Y_res):\n",
        "\tind1 = np.where(Y_res == 0)[0]\n",
        "\tXH = X_res[ind1,:] # honest samples\n",
        "\tYH = np.zeros(ind1.shape[0])\n",
        "\tind_H = int(math.ceil(ind1.shape[0]*4/5))\n",
        "\tXH_TR = XH[0:ind_H,:]\n",
        "\tYH_TR = YH[0:ind_H]\n",
        "\tXH_TST = XH[ind_H:,:]\n",
        "\tYH_TST = YH[ind_H:] \n",
        "\tXC = np.concatenate((at1,at3,at4,at5), axis=0) # all attacks\n",
        "\tYC = np.ones(XC.shape[0]) \n",
        "\tind_C = int(math.ceil(536*4/5))\n",
        "\tXC_TR = np.concatenate((at1[0:ind_C,:],at3[0:ind_C,:],at4[0:ind_C,:],at5[0:ind_C,:]), axis=0) # training cheating samples\n",
        "\tYC_TR = YC[0:XC_TR.shape[0]]\n",
        "\tXC_TST = np.concatenate((at1[ind_C:,:],at3[ind_C:,:],at4[ind_C:,:],at5[ind_C:,:]), axis=0) # test cheating samples\n",
        "\tYC_TST = YC[XC_TR.shape[0]:]     \n",
        "\tXTR = np.concatenate((XH_TR,XC_TR), axis=0)  \n",
        "\tYTR = np.concatenate((YH_TR,YC_TR), axis=0) \n",
        "\tind = list(range(XTR.shape[0]))  #random shuffling the training data\n",
        "\tnp.random.shuffle(ind)\n",
        "\tXTR = XTR[ind]\n",
        "\tYTR = YTR[ind]\n",
        "\tXTST = np.concatenate((XH_TST,XC_TST), axis=0)   \n",
        "\tYTST = np.concatenate((YH_TST,YC_TST), axis=0)   \n",
        "\treturn XTR,YTR,XTST,YTST"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9qZUpGNAPFN",
        "outputId": "afe8d510-a681-41d2-f298-85edd7227d47"
      },
      "source": [
        "\n",
        "print (\"Loading Data ...\")\n",
        "D = np.array(list(csv.reader(open(fIn, \"rt\"), delimiter=\",\"))).astype(\"float\")\n",
        "D = np.loadtxt(fIn, delimiter=\",\")\n",
        "D = np.transpose(D[1:,:])\n",
        "x_tr = open('Xtr_all_200.csv','ab')\n",
        "y_tr = open('Ytr_all_200.csv','ab')\n",
        "x_tst = open('Xtst_all_200.csv','ab')\n",
        "y_tst = open('Ytst_all_200.csv','ab')\n",
        "\n",
        "for d in D: # for each user\n",
        "  if i==200:\n",
        "    break\n",
        "  i+=1\n",
        "  print(i)\n",
        "  file_name = 'Users/'+str(i)+'.pkl'\n",
        "  #if training data for that user exist load it (balanced data set for each user has both honest and malicious samples) \n",
        "  if os.path.exists(file_name):\n",
        "    [XTR,YTR,XTST,YTST] = pickle.load(open(file_name, 'rb'))\n",
        "  else:\n",
        "    d = d.reshape(536,48) # customer data arranged in matrix: rows = days, columns = time\n",
        "      ############################################################## \n",
        "      # Creating attacks\n",
        "    at1, at3, at4, at5 = create_attacks(d)\n",
        "      ##############################################################    \n",
        "      # Over-sampling\n",
        "    X = np.concatenate((d,at1,at3,at4,at5), axis=0) # all data = honest + attacks\n",
        "    y1 = np.zeros(536)\n",
        "    y2 = np.ones(536*6)\n",
        "    Y = np.concatenate((y1,y2), axis=0)\n",
        "    ada = ADASYN(ratio='minority',random_state=42)\n",
        "    X_res, Y_res = ada.fit_sample(X,Y) # over-sampled data\n",
        "      \n",
        "      ##############################################################    \n",
        "      # Split data to train 2/3 and test 1/3\n",
        "    XTR,YTR,XTST,YTST = split_train_test(X_res,Y_res) \n",
        "    #pickle.dump([XTR,YTR,XTST,YTST], open(file_name, 'wb'))\n",
        "      ############################################################## \t\t\t\t\n",
        "  np.savetxt(x_tr, XTR, delimiter=\",\")\n",
        "  np.savetxt(y_tr, YTR, delimiter=\",\")\n",
        "  np.savetxt(x_tst, XTST, delimiter=\",\")\n",
        "  np.savetxt(y_tst, YTST, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}