{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepdata.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2cwcSazMeqayXBwBO/90q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayhd3/NSF-REU-2021/blob/main/prepdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGzgpdTx_30N"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('Irish_dataset.csv'):\n",
        "  !gdown --id 1OFQo6CmBPwn3FRB5EsRm9t6wba_5Lcr9"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6oMnJwF_u5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4bd50e-6e36-4e6e-a30a-b23ac37b87a4"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM, Merge, GRU\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.utils import shuffle\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "\n",
        "i = 0 # counter for customers\n",
        "a_dr = 0 # detection rate initialization\n",
        "a_fr = 0 # false rejection initialization \n",
        "a_fa = 0 # false acceptance initialization\n",
        "fIn = \"Irish_dataset.csv\" \n",
        "# fOt1 = sys.argv[2]\n",
        "np.random.seed(1234) # Set the random seed for numpy\n",
        "# flog1 = open(fOt1,'w')\n",
        "scores = list()\n",
        "\n",
        "# creat six types of attacks\n",
        "def create_attacks(d):\n",
        "\talpha1 = random.uniform(0.1,0.6)\n",
        "\tat1 = np.dot(alpha1,d) #1st attack type\t\n",
        "\talpha2 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat3 = np.multiply(alpha2,d) # 3rd attack type\n",
        "\tavg = np.mean(d,axis=1) # customer daily average consumption\n",
        "\tavg_m = np.diag(avg) # in diagonal matrix for multiplication  \n",
        "\talpha3 = np.random.uniform(low=0.1, high=0.6, size=(536,48))\n",
        "\tat4 = np.dot(avg_m,alpha3) # 4th attack type\n",
        "\talpha4 = np.ones((536,48))\n",
        "\tat5 = np.dot(avg_m,alpha4) # 5th attack type\t\n",
        "\treturn at1,at3,at4,at5\n",
        "\n",
        "# Split the data to train and test\n",
        "def split_train_test(X_res,Y_res):\n",
        "\tind1 = np.where(Y_res == 0)[0]\n",
        "\tXH = X_res[ind1,:] # honest samples\n",
        "\tYH = np.zeros(ind1.shape[0])\n",
        "\tind_H = int(math.ceil(ind1.shape[0]*4/5))\n",
        "\tXH_TR = XH[0:ind_H,:]\n",
        "\tYH_TR = YH[0:ind_H]\n",
        "\tXH_TST = XH[ind_H:,:]\n",
        "\tYH_TST = YH[ind_H:] \n",
        "\tXC = np.concatenate((at1,at3,at4,at5), axis=0) # all attacks\n",
        "\tYC = np.ones(XC.shape[0]) \n",
        "\tind_C = int(math.ceil(536*4/5))\n",
        "\tXC_TR = np.concatenate((at1[0:ind_C,:],at3[0:ind_C,:],at4[0:ind_C,:],at5[0:ind_C,:]), axis=0) # training cheating samples\n",
        "\tYC_TR = YC[0:XC_TR.shape[0]]\n",
        "\tXC_TST = np.concatenate((at1[ind_C:,:],at3[ind_C:,:],at4[ind_C:,:],at5[ind_C:,:]), axis=0) # test cheating samples\n",
        "\tYC_TST = YC[XC_TR.shape[0]:]     \n",
        "\tXTR = np.concatenate((XH_TR,XC_TR), axis=0)  \n",
        "\tYTR = np.concatenate((YH_TR,YC_TR), axis=0) \n",
        "\tind = list(range(XTR.shape[0]))  #random shuffling the training data\n",
        "\tnp.random.shuffle(ind)\n",
        "\tXTR = XTR[ind]\n",
        "\tYTR = YTR[ind]\n",
        "\tXTST = np.concatenate((XH_TST,XC_TST), axis=0)   \n",
        "\tYTST = np.concatenate((YH_TST,YC_TST), axis=0)   \n",
        "\treturn XTR,YTR,XTST,YTST"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9qZUpGNAPFN",
        "outputId": "a3a26484-3f26-4846-8ad3-3c5d4829ce6a"
      },
      "source": [
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils \n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import numpy as np\n",
        "from keras.layers import Dense, GRU\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score , recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "import math\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print (\"Loading Data ...\")\n",
        "D = np.loadtxt(fIn, delimiter=\",\").T[:,1:].astype(\"float\")\n",
        "print(D)\n",
        "#D = np.transpose(D[1:,:])\n",
        "x_tr = open('Xtr_all_200.csv','ab')\n",
        "y_tr = open('Ytr_all_200.csv','ab')\n",
        "x_tst = open('Xtst_all_200.csv','ab')\n",
        "y_tst = open('Ytst_all_200.csv','ab')\n",
        "\n",
        "for d in D: # for each user\n",
        "  if i==200:\n",
        "    break\n",
        "  i+=1\n",
        "  if i%10 == 0:\n",
        "    print(i)\n",
        "  file_name = 'Users/'+str(i)+'.pkl'\n",
        "  #if training data for that user exist load it (balanced data set for each user has both honest and malicious samples) \n",
        "  if os.path.exists(file_name):\n",
        "    [XTR,YTR,XTST,YTST] = pickle.load(open(file_name, 'rb'))\n",
        "  else:\n",
        "    d = d.reshape(536,48) # customer data arranged in matrix: rows = days, columns = time\n",
        "      ############################################################## \n",
        "      # Creating attacks\n",
        "    at1, at3, at4, at5 = create_attacks(d)\n",
        "      ##############################################################    \n",
        "      # Over-sampling\n",
        "    X = np.concatenate((d,at1,at3,at4,at5), axis=0) # all data = honest + attacks\n",
        "    y1 = np.zeros(536)\n",
        "    y2 = np.ones(536*6)\n",
        "    Y = np.concatenate((y1,y2), axis=0)\n",
        "    ada = ADASYN(ratio='minority',random_state=42)\n",
        "    fit = min(len(X),len(Y))\n",
        "    X_res, Y_res = ada.fit_sample(X[:fit],Y[:fit]) # over-sampled data\n",
        "      \n",
        "      ##############################################################    \n",
        "      # Split data to train 2/3 and test 1/3\n",
        "    XTR,YTR,XTST,YTST = split_train_test(X_res,Y_res) \n",
        "    #pickle.dump([XTR,YTR,XTST,YTST], open(file_name, 'wb'))\n",
        "      ############################################################## \t\t\t\t\n",
        "  np.savetxt(x_tr, XTR, delimiter=\",\")\n",
        "  np.savetxt(y_tr, YTR, delimiter=\",\")\n",
        "  np.savetxt(x_tst, XTST, delimiter=\",\")\n",
        "  np.savetxt(y_tst, YTST, delimiter=\",\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data ...\n",
            "[[0.362 0.064 0.119 ... 0.084 0.116 0.147]\n",
            " [0.692 0.381 0.38  ... 0.898 0.802 0.835]\n",
            " [1.31  1.142 1.218 ... 1.819 1.422 1.393]\n",
            " ...\n",
            " [0.289 0.187 0.12  ... 0.73  1.515 0.935]\n",
            " [0.584 0.207 0.196 ... 0.096 0.165 0.097]\n",
            " [0.148 0.148 0.134 ... 0.486 0.318 0.332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il53yObaKkBl",
        "outputId": "804da850-3605-433f-b25c-f432e1512bf3"
      },
      "source": [
        "# load the data\n",
        "def data():\n",
        "    Xtr = pd.read_csv(\"Xtr_all_200.csv\", sep=',', header=None)\n",
        "    Xtst = pd.read_csv(\"Xtst_all_200.csv\", sep=',', header=None)\n",
        "    Ytr = pd.read_csv(\"Ytr_all_200.csv\", sep=',', header=None)\n",
        "    Ytst = pd.read_csv(\"Ytst_all_200.csv\", sep=',', header=None)\n",
        "    \n",
        "    scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "    xtr = scaler.transform(Xtr)  \n",
        "    xtst = scaler.transform(Xtst)  \n",
        "\n",
        "    xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "    xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "    nb_classes = 2\n",
        "\n",
        "    print (\"Number of training samples is: \",xtr.shape[0])\n",
        "    print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "    ytr = to_categorical(Ytr, nb_classes)\n",
        "    ytst = to_categorical(Ytst, nb_classes)\n",
        "    \n",
        "    return xtr,ytr,xtst,ytst\n",
        "    \n",
        "xtr,ytr,xtst,ytst=data()\n",
        "\n",
        "num_classes = 2 \n",
        "epochs = 150     #should be set\n",
        "\n",
        "input_shape = (48,1)\n",
        "model = Sequential()\n",
        "model.add(GRU(32, input_shape=(input_shape), activation='relu'))  \n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))    \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.2)  #learning_rte\n",
        "\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(xtst, ytst))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples is:  685621\n",
            "Number of test samples is:  171110\n",
            "Epoch 1/150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-gXSYI9K8Md"
      },
      "source": [
        "# load the data\n",
        "def data():\n",
        "    Xtr = pd.read_csv(\"Xtr_all_200.csv\", sep=',', header=None)\n",
        "    Xtst = pd.read_csv(\"Xtst_all_200.csv\", sep=',', header=None)\n",
        "    Ytr = pd.read_csv(\"Ytr_all_200.csv\", sep=',', header=None)\n",
        "    Ytst = pd.read_csv(\"Ytst_all_200.csv\", sep=',', header=None)\n",
        "    \n",
        "    scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "    xtr = scaler.transform(Xtr)  \n",
        "    xtst = scaler.transform(Xtst)  \n",
        "\n",
        "    xtr = xtr.reshape(xtr.shape[0],xtr.shape[1],1)\n",
        "    xtst = xtst.reshape(xtst.shape[0],xtst.shape[1],1)\n",
        "\n",
        "    nb_classes = 2\n",
        "\n",
        "    print (\"Number of training samples is: \",xtr.shape[0])\n",
        "    print (\"Number of test samples is: \",xtst.shape[0])\n",
        "\n",
        "    ytr = to_categorical(Ytr, nb_classes)\n",
        "    ytst = to_categorical(Ytst, nb_classes)\n",
        "    \n",
        "    return xtr,ytr,xtst,ytst\n",
        "    \n",
        "xtr,ytr,xtst,ytst=data()\n",
        "\n",
        "input_shape = (48,1)\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=7, activation='relu', input_shape=(input_shape)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))    \n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam(lr=0.001), metrics=['accuracy'])\n",
        "model.fit(xtr, ytr, epochs=epochs, verbose=2, batch_size=128,validation_split=0.3)  #learning_rte\n",
        "    \n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(xtst, ytst))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}